{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from pyspark.sql.functions import concat, to_timestamp, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Define the specific time range\n",
    "start_time = \"12:45:00\"\n",
    "end_time = \"13:45:00\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/01/04 16:16:32 WARN Utils: Your hostname, J4Hp resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/01/04 16:16:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/04 16:16:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/04 16:16:47 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data loaded successfully.\n",
      "DataFrame[TICKET_ISSUE_DATE: string, TICKET_ISSUE_TIME: timestamp, FROM_STOP_NAME: string, TO_STOP_NAME: string, TOTAL_PASSENGER: int, ROUTE_NAME: string, NO_OF_ADULT: int, NO_OF_CHILD: int]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as spark_sum, col, hour, concat_ws, to_date, date_format\n",
    "from pyspark.sql.functions import to_timestamp, concat, col, lit, date_format, expr\n",
    "# Stop any existing Spark sessions to avoid conflicts\n",
    "if SparkSession._instantiatedSession:\n",
    "    SparkSession._instantiatedSession.stop()\n",
    "\n",
    "# Step 1: Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataProcessing\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load your CSV file into a Spark DataFrame\n",
    "try:\n",
    "    data = spark.read.csv(\"filtered_june.csv\", header=True, inferSchema=True)\n",
    "    print(\"CSV data loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    spark.stop()\n",
    "    raise  # Stop the script if loading fails\n",
    "\n",
    "\n",
    "# Filter rows where ROUTE_ID is 'acwXkRFM'\n",
    "# data = data.filter(col(\"ROUTE_ID\") == 'acwXkRFM')\n",
    "# Step 2: Format TICKET_ISSUE_TIME as a string in \"HH:mm:ss\" format (if not already) and combine date and time\n",
    "print(data)\n",
    "data = data.withColumn(\"TICKET_ISSUE_TIME_STR\", date_format(col(\"TICKET_ISSUE_TIME\"), \"HH:mm:ss\"))\n",
    "# Filter rows within the specific time range\n",
    "# Show data which has time other than start_time and end_time\n",
    "\n",
    "data = data.filter((col(\"TICKET_ISSUE_TIME_STR\") >= start_time) & (col(\"TICKET_ISSUE_TIME_STR\") <= end_time))\n",
    "# Calculate the total number of days in the dataset\n",
    "total_days = data.select(to_date(col(\"TICKET_ISSUE_DATE\")).alias(\"date\")).distinct().count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LIMIT_OF_TOP_BUS_STOPS = 600\n",
    "MIN_AVG_THRESHOLD = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import folium\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "\n",
    "# Aggregate data to get total passenger count per bus stop within the time range\n",
    "# Sort by total passengers in descending order and select the top 20\n",
    "top_bus_stops = (\n",
    "    data.groupBy(\"FROM_STOP_NAME\")\n",
    "    .agg(F.sum(\"TOTAL_PASSENGER\").alias(\"TOTAL_PASSENGER\"))\n",
    "    .withColumn(\"AVERAGE_PASSENGER\", F.col(\"TOTAL_PASSENGER\") / total_days)\n",
    "    .filter(F.col(\"AVERAGE_PASSENGER\") >= MIN_AVG_THRESHOLD)\n",
    "    .orderBy(\"TOTAL_PASSENGER\", ascending=False)\n",
    "    .limit(LIMIT_OF_TOP_BUS_STOPS)\n",
    "    .collect()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stop the Spark session at the end\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ] 100.0%\n",
      "Geocoding Successes: 303\n",
      "Geocoding Failures: 166\n",
      "Failed to geocode the following bus stops:\n",
      "['Statue Sbi (outside South India)', 'Chakai (outside South India)', 'World Market (outside South India)', 'Easwara Vilasam Cotton Hill School', 'Rotary (outside South India)', 'Mangalapuram East', '16th Mile (outside South India)', 'Aiyroopara', 'Civil Station Junction Kudappanakunnu', 'Mamam (outside South India)', 'Pullampara Palam', 'Panangodu Junction', 'Edapazhanji Pangode Fish Market', 'Kachani Junction', 'Kalliyoor Grama Panchayath Office', 'Vellarada Ksrtc Depot', 'Thekkada Junction', 'Beemapalli Back', 'East Mukkola', 'Kazhakkoottam Railway Station', 'Kudappanakunnu Bank Junction', 'Pettah Pallimukku Junction', 'Mananthala (outside South India)', 'Titanium (outside South India)', 'Irinchayam Steps', 'Puthukurichi Post Office', 'Sainik School (outside South India)', 'Mithrumala', 'Kanjirampara Junction', 'St Andrews (outside South India)', 'Punchakkari', 'Kottarakkari', 'Kayalkara', 'Vazhavilapaalam', 'Kattadimukku', 'Water Tank Pallithura', 'Poovanathin Moodu', 'Thrikkannapuram (outside South India)', 'Chempoor Junction', 'Puthiyathura Junction', '6th Stone Vazhayila', 'Oorambu', 'Pazhayakada Junction', 'Irumpa', 'Block Junction Venkavila', 'Poovar Junction', 'Museum (outside South India)', 'Military Hospital Pangodu', 'Vamanapuram Phc', 'Venkavila Junction', 'Chadayamangalam Bus Stand', 'Pezhummoodu Junction', 'Vavara Ambalam', 'Veli Church (outside South India)', 'Thumpod', 'Cheru Vettukadu', 'Karipur (outside South India)', 'Paruthikkuzhy', 'Chenkavila Junction', 'East Fort South Stand 2', 'Olathanni Junction', 'Maithani (outside South India)', 'Channakara', 'Pappanchani Junction', 'Postoffice Junction Parassala', 'Arumanior Junction', 'Vandannur', 'Pachaloor Chudukadu Temple', 'Ks Road Kovalam Junction', 'Meenakal', 'Narmadha Toll Junction', 'Melathumele', 'Mele Aramthanam', 'Valliya Kattayikkal', 'Madhupalam', 'Government Lp School Marayamuttom', 'Ookkodu', 'Seematty', 'Konkalam Junction', 'Pozhiyoor Junction', 'Pulinkudy', 'Chettikulangara Sreedevi Temple', 'J T S Mancha (outside South India)', 'Malayam High School', 'Nannattukavu A', 'Puthuveettumele', 'Killy (outside South India)', 'Post Office Manivila', 'Radio Park Junction', 'Karikkakom Vayanasala', 'Kalliyodu', 'Bakery Junction (outside South India)', '10th Stone Patham Kallu', 'Chalakuzhy', 'Vithura Kalung Junction', 'Concordia Lutheran High School (outside South India)', 'Chittalloor Temple', 'Villant (outside South India)', 'Azhikode (outside South India)', 'Scooter Factory (outside South India)', 'Amaravila Thannimoodu', '28th Mile (outside South India)', 'Technopark Front Gate', 'Karumom Vishnu Temple', 'Government Press Mannanthala', 'Devi Nagar (outside South India)', 'Chekkalamukku', 'Iti Chakai', 'Medical College Sat', 'Arattukuzhi', 'Punnakkamugal Junction', 'Aramada (outside South India)', 'Urban P H C Muttada', 'Nilama (outside South India)', 'Aramada Good Shepherd', 'Chadayamankalam', 'Vellumannady Palam', 'Chengalloor Hll', 'Poomalliyoorkonam', 'Bhagat Singh Road (outside South India)', 'Madathikonam', 'Vallamcode Junction', 'Pannimala', 'Thiruvallom School', 'Perunelli', 'Thekkada Stpes', 'Pachira (outside South India)', 'Sree Chithra Medical College', 'Aruvattukonam Point', 'Muthalapozhi Palam', 'Alumuttu', 'Pammathikeezhu', 'Koothali (outside South India)', 'Near Mannammoola Palam', 'Technical H S S (outside South India)', \"St Joseph's School Muttada\", 'Panagod', 'Muttada St Joseph School', 'Pazhaya Mukkola Or Mananthala East', 'Oonnampara', 'Kulappada Market/eliyavoor', 'Fci (outside South India)', 'Mele Venchavode Or Imb Hospital']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Latitude and Longitude bounds for South India\n",
    "SOUTH_INDIA_LAT_MIN = 8.0\n",
    "SOUTH_INDIA_LAT_MAX = 14.5\n",
    "SOUTH_INDIA_LON_MIN = 76.0\n",
    "SOUTH_INDIA_LON_MAX = 85.0\n",
    "\n",
    "# File to store previously geocoded bus stops\n",
    "GEO_CACHE_FILE = 'geocoded_stops.json'\n",
    "FAILURE_CACHE_FILE = 'geocoding_failures.json'\n",
    "\n",
    "# Function to load previously cached geocoded data from a JSON file\n",
    "def load_geocoded_data():\n",
    "    try:\n",
    "        with open(GEO_CACHE_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "# Function to save geocoded data to a JSON file\n",
    "def save_geocoded_data(data):\n",
    "    with open(GEO_CACHE_FILE, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Function to check if a location is in South India\n",
    "def is_in_south_india(latitude, longitude):\n",
    "    return (SOUTH_INDIA_LAT_MIN <= latitude <= SOUTH_INDIA_LAT_MAX) and (SOUTH_INDIA_LON_MIN <= longitude <= SOUTH_INDIA_LON_MAX)\n",
    "\n",
    "def load_geocoded_failures():\n",
    "    try:\n",
    "        with open(FAILURE_CACHE_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "# Prepare data for geocoding (replace 'top_bus_stops' with your actual data)\n",
    "bus_stops_data = [{\"stop_name\": row[\"FROM_STOP_NAME\"], \"passenger_count\": row[\"AVERAGE_PASSENGER\"]} for row in top_bus_stops]\n",
    "\n",
    "# Initialize the geocoder\n",
    "geolocator = Nominatim(user_agent=\"bus_stop_locator\")\n",
    "\n",
    "# Load previously geocoded data from the cache\n",
    "cached_data = load_geocoded_data()\n",
    "cached_failure_data = load_geocoded_failures()\n",
    "\n",
    "# Initialize counters for success and failure\n",
    "success_count = 0\n",
    "failure_count = 0\n",
    "failures = []\n",
    "\n",
    "# Function to print the progress bar\n",
    "def print_progress_bar(iteration, total, bar_length=40):\n",
    "    progress = iteration / total\n",
    "    arrow = '=' * int(round(progress * bar_length) - 1)\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    percent = round(progress * 100, 1)\n",
    "    sys.stdout.write(f'\\r[{arrow}{spaces}] {percent}%')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Geocode each bus stop with exponential backoff\n",
    "for i, stop in enumerate(bus_stops_data):\n",
    "    stop_name = stop[\"stop_name\"]\n",
    "    \n",
    "    # Skip if the stop is in the failures list\n",
    "    if stop_name in cached_data:\n",
    "        # Use cached data\n",
    "        stop[\"latitude\"] = cached_data[stop_name][\"latitude\"]\n",
    "        stop[\"longitude\"] = cached_data[stop_name][\"longitude\"]\n",
    "        success_count += 1  # Increment success count for cached data\n",
    "        print_progress_bar(i + 1, len(bus_stops_data))  # Update progress bar for this stop\n",
    "        continue  # Skip geocoding since it's already cached\n",
    "    elif stop_name in cached_failure_data:\n",
    "        # Skip geocoding if it previously failed\n",
    "        failure_count += 1\n",
    "        print_progress_bar(i + 1, len(bus_stops_data))  # Update progress bar for this stop\n",
    "        continue\n",
    "\n",
    "    # If not cached, geocode this stop\n",
    "    retries = 0  # Counter for retry attempts\n",
    "    while retries < 2:\n",
    "        try:\n",
    "            # Geocode the stop with a timeout\n",
    "            location = geolocator.geocode(stop_name, timeout=20)\n",
    "\n",
    "            if location:\n",
    "                latitude = location.latitude\n",
    "                longitude = location.longitude\n",
    "\n",
    "                # Check if the coordinates are within South India's bounds\n",
    "                if is_in_south_india(latitude, longitude):\n",
    "                    stop[\"latitude\"] = latitude\n",
    "                    stop[\"longitude\"] = longitude\n",
    "                    # Save the geocoded result in the cache\n",
    "                    cached_data[stop_name] = {\"latitude\": stop[\"latitude\"], \"longitude\": stop[\"longitude\"]}\n",
    "                    success_count += 1  # Increment success count\n",
    "                else:\n",
    "                    # If outside South India, mark as None\n",
    "                    stop[\"latitude\"] = None\n",
    "                    stop[\"longitude\"] = None\n",
    "                    failure_count += 1  # Increment failure count\n",
    "                    failures.append(f\"{stop_name} (outside South India)\")\n",
    "            else:\n",
    "                stop[\"latitude\"] = None\n",
    "                stop[\"longitude\"] = None\n",
    "                failure_count += 1  # Increment failure count\n",
    "                failures.append(stop_name)\n",
    "\n",
    "            break  # Exit the retry loop on success\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error geocoding {stop_name}: {e}\")\n",
    "            stop[\"latitude\"] = None\n",
    "            stop[\"longitude\"] = None\n",
    "            failures.append(stop_name)\n",
    "            failure_count += 1  # Increment failure count\n",
    "\n",
    "            # Exponential backoff\n",
    "            backoff_time = min(2 ** retries + random.uniform(0, 1), 30)  # max backoff of 30 seconds\n",
    "            print(f\"Retrying {stop_name} in {backoff_time:.2f} seconds...\")\n",
    "            time.sleep(backoff_time)  # Sleep exponentially between retries\n",
    "\n",
    "    # If retries are exhausted, skip to the next stop\n",
    "    if retries == 2:\n",
    "        print(f\"Failed to geocode {stop_name} after {retries} retries.\")\n",
    "    \n",
    "    # Update progress bar\n",
    "    print_progress_bar(i + 1, len(bus_stops_data))\n",
    "\n",
    "# Save the updated geocoded data to the cache file\n",
    "save_geocoded_data(cached_data)\n",
    "\n",
    "# Output the number of successes and failures\n",
    "print(f\"\\nGeocoding Successes: {success_count}\")\n",
    "print(f\"Geocoding Failures: {failure_count}\")\n",
    "if failures:\n",
    "    print(\"Failed to geocode the following bus stops:\")\n",
    "    print(failures)\n",
    "\n",
    "    # Function to save geocoding failures to a JSON file\n",
    "    def save_geocoding_failures(failures):\n",
    "        with open(FAILURE_CACHE_FILE, 'w') as f:\n",
    "            json.dump(failures, f, indent=4)\n",
    "\n",
    "    # Save the geocoding failures to the cache file\n",
    "    save_geocoding_failures(failures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<branca.element.Element at 0x7fe3eee86da0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from folium.plugins import HeatMap\n",
    "import folium\n",
    "import pandas as pd\n",
    "from folium import Icon\n",
    "from folium.plugins import MarkerCluster\n",
    "import numpy as np\n",
    "\n",
    "# Filter out stops without coordinates\n",
    "stops_with_coords = [stop for stop in bus_stops_data if 'latitude' in stop and 'longitude' in stop and stop[\"latitude\"] is not None and stop[\"longitude\"] is not None]\n",
    "\n",
    "# Convert to Pandas DataFrame for easier handling with Folium\n",
    "stops_df = pd.DataFrame(stops_with_coords)\n",
    "\n",
    "# Initialize a Folium map centered around an average location\n",
    "map_center = [8.4869, 76.9529]\n",
    "m = folium.Map(location=map_center, tiles=\"CartoDB positron\", zoom_start=13, min_zoom=8, max_zoom=18)\n",
    "\n",
    "# Logarithmic transformation of passenger counts for better contrast in markers\n",
    "stops_df['log_passenger_count'] = np.log1p(stops_df['passenger_count'])\n",
    "\n",
    "# Prepare data for HeatMap using actual passenger counts for intensity\n",
    "heat_data = []\n",
    "for _, row in stops_df.iterrows():\n",
    "    heat_data.append([row[\"latitude\"], row[\"longitude\"], row[\"passenger_count\"]])  # Using actual count for heatmap\n",
    "\n",
    "# Create the HeatMap layer with adjusted visual settings\n",
    "HeatMap(\n",
    "    heat_data,\n",
    "    min_opacity=0.3,  # Set minimum opacity for better visibility (not too faint)\n",
    "    max_opacity=0.7,  # Set maximum opacity for a more subtle heatmap\n",
    "    radius=25,        # Adjust radius size to balance between clarity and overlap\n",
    "    blur=18,          # Moderate blur to avoid excessive smoothing\n",
    "    gradient={        # Reduced 5-color gradient scale for better distinction\n",
    "        0.2: 'blue',   # Low density -> blue\n",
    "        0.4: 'green',  # Medium-low density -> green\n",
    "        0.6: 'yellow', # Medium-high density -> yellow\n",
    "        0.8: 'orange', # High density -> orange\n",
    "        1.0: 'red',    # Very high density -> red\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Create a MarkerCluster for the stops (useful for closely spaced stops)\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# Define color mapping for passenger counts (using log-transformed values for marker colors)\n",
    "def get_marker_color(log_count):\n",
    "    if log_count < 3.1:   # 0 to 20 passengers\n",
    "        return 'blue'      # Low density -> blue\n",
    "    elif log_count < 5.8:  # 21 to 500 passengers\n",
    "        return 'green'     # Medium density -> green\n",
    "    elif log_count < 6.9:  # 501 to 1000 passengers\n",
    "        return 'orange'    # High density -> orange\n",
    "    else:                  # 1000+ passengers\n",
    "        return 'red'       # Very high density -> red\n",
    "\n",
    "\n",
    "# Add popups and clustered markers for bus stops with their name and transformed passenger count\n",
    "for _, row in stops_df.iterrows():\n",
    "    # Get the color based on the transformed passenger count\n",
    "    color = get_marker_color(row['log_passenger_count'])\n",
    "    \n",
    "    marker = folium.Marker(\n",
    "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "        popup=f\"<b>{row['stop_name']}</b><br>Passenger count: {row['passenger_count']}<br>Log Transformed: {row['log_passenger_count']:.2f}\",\n",
    "        tooltip=row[\"stop_name\"],\n",
    "        icon=Icon(color=color, icon=\"fa-users\", prefix=\"fa\"),  # Apply color dynamically\n",
    "    )\n",
    "    marker.add_to(marker_cluster)  # Add to MarkerCluster for better organization\n",
    "\n",
    "# Create a legend HTML for color decoding (simplified and smaller)\n",
    "legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; left: 50px; width: 240px; height: 160px; \n",
    "                background-color: white; border: 2px solid grey; padding: 20px; \n",
    "                z-index: 9999; font-size: 10px; border-radius: 8px;\">\n",
    "        <b>Passenger Density Legend</b><br>\n",
    "        <i style=\"background: blue; width: 20px; height: 20px; display: inline-block;\"></i> Low Density (0 - 20 passengers)<br>\n",
    "        <i style=\"background: green; width: 20px; height: 20px; display: inline-block;\"></i> Medium Density (21 - 500 passengers)<br>\n",
    "        <i style=\"background: orange; width: 20px; height: 20px; display: inline-block;\"></i> High Density (501 - 1000 passengers)<br>\n",
    "        <i style=\"background: red; width: 20px; height: 20px; display: inline-block;\"></i> Very High Density (1000+ passengers)\n",
    "    </div>\n",
    "'''\n",
    "\n",
    "# Add the legend to the map\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save map to an HTML file\n",
    "m.save(\"passenger_boarding_density_fff.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
