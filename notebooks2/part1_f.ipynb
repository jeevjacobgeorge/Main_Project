{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ldvBq1S1amdQ"
      },
      "outputs": [],
      "source": [
        "month = \"joined\"\n",
        "# file_path = \"/content/drive/My Drive/filtered_pred_\"+month+\"2024.csv\"\n",
        "file_path = \"data/filtered_pred_\"+month+\"2024.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Nv5FR-aoHu",
        "outputId": "650affa3-e85d-48f2-f513-72097938712a"
      },
      "outputs": [],
      "source": [
        "# # # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_7c5TnZ-Am9r"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Stop any existing Spark session\n",
        "# Step 1: Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigDataProcessing\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "# Step 2: Load your CSV file into a Spark DataFrame\n",
        "data = spark.read.csv(file_path, header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1qTrCWETohx",
        "outputId": "c5609323-6b11-4c55-befd-8d868ac4a040"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "data = data.withColumn(\"DATE_HOUR\", F.concat_ws(\" \", F.col(\"DATE\"), F.col(\"HOUR\")))\n",
        "data = data.drop(\"DATE\",\"HOUR\",\"ROUTE_NAME\")\n",
        "data.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "33BJaFqVTTcw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum as spark_sum\n",
        "\n",
        "\n",
        "data_grouped = data.groupBy(\"DATE_HOUR\").agg(\n",
        "    spark_sum(\"NO_OF_ADULT\").alias(\"NO_OF_ADULT\"),\n",
        "    spark_sum(\"NO_OF_CHILD\").alias(\"NO_OF_CHILD\")\n",
        ")\n",
        "\n",
        "data_grouped = data_grouped.orderBy(\"DATE_HOUR\")\n",
        "data_grouped = data_grouped.withColumn(\"TOTAL_PASSENGERS\", F.col(\"NO_OF_ADULT\") + F.col(\"NO_OF_CHILD\"))\n",
        "data_grouped.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_grouped.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 7: Convert 'DATE_HOUR' to datetime format, then set as the index\n",
        "df['DATE_HOUR'] = pd.to_datetime(df['DATE_HOUR'], format='%Y-%m-%d %H')\n",
        "df.set_index('DATE_HOUR', inplace=True)\n",
        "df.sort_index(inplace=True)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = sc.fit_transform(df)\n",
        "print(scaled_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpsJK9rrA4rc"
      },
      "outputs": [],
      "source": [
        "# Step 7: Scale the data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "\n",
        "# print(scaled_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUN6wimkA9Hc"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnWbn51vBCJb"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Step 9: Prepare Train and Test Data\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(scaled_data) * split_ratio)\n",
        "# Create Train and Test datasets\n",
        "Train = scaled_data[:split_index]\n",
        "Test = scaled_data[split_index:]\n",
        "# Save the Test dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFcyNPTWBDb3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "# Step 10: Create TimeseriesGenerator for training data\n",
        "n_input = 24  # Use the last 24 hours for prediction\n",
        "n_features = 2  # Number of features (NO_OF_ADULT and NO_OF_CHILD)\n",
        "\n",
        "generator = TimeseriesGenerator(Train, Train, length=n_input, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPY9MUVKBFzF",
        "outputId": "9c714242-31e2-4010-9e17-29f39efcfc9e"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
        "# Define the LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(units=100, activation='tanh', return_sequences=True), input_shape=(n_input, n_features)),\n",
        "    Bidirectional(LSTM(units=50, activation='tanh')),\n",
        "    Dense(units=n_features, activation='linear')  # 'linear' is appropriate for regression\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMZqjnwEBKow"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kAQ2q1yBL1x",
        "outputId": "983e0f9f-e020-4709-c66c-5d01c3d79c43"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# model training\n",
        "# Train the model\n",
        "epochs = 29  # More epochs can lead to better performance\n",
        "\n",
        "# Add EarlyStopping callback to stop training if the validation loss does not improve\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=4, restore_best_weights=True)\n",
        "\n",
        "# Train the model with the EarlyStopping callback\n",
        "history = model.fit(generator, epochs=epochs, verbose=1, callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBTRq2gjIrBx"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/My Drive/'+month+'.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_Lf7wPOHqElW",
        "outputId": "067df457-edf3-4ab9-cb4a-bfd5054e5916"
      },
      "outputs": [],
      "source": [
        "month"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
