{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install keras \n",
        "# !pip install scikit-learn\n",
        "# !pip install matplotlib\n",
        "# !pip install tensorflow\n",
        "# !pip install numpy<2\n",
        "# !pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ldvBq1S1amdQ"
      },
      "outputs": [],
      "source": [
        "month = \"June\"\n",
        "file_path = \"data/filtered_pred_\"+month+\"2024.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L_KUfZl_Ak2n"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_7c5TnZ-Am9r"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum as spark_sum, col, hour, concat_ws, to_date, date_format\n",
        "# Stop any existing Spark session\n",
        "# Step 1: Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigDataProcessing\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "# Step 2: Load your CSV file into a Spark DataFrame\n",
        "data = spark.read.csv(file_path, header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dTrQ0_zuAxhr"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = data.groupBy(\"DATE_HOUR\") \\\n",
        "    .agg(\n",
        "        spark_sum(\"NO_OF_ADULT\").alias(\"NO_OF_ADULT\"),\n",
        "        spark_sum(\"NO_OF_CHILD\").alias(\"NO_OF_CHILD\")\n",
        "    )\n",
        "df = data.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KD045iNrA1_l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Step 7: Convert 'DATE_HOUR' to datetime format, then set as the index\n",
        "df['DATE_HOUR'] = pd.to_datetime(df['DATE_HOUR'], format='%Y-%m-%d %H')\n",
        "df.set_index('DATE_HOUR', inplace=True)\n",
        "df.sort_index(inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cpsJK9rrA4rc"
      },
      "outputs": [],
      "source": [
        "# Step 7: Scale the data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "\n",
        "# print(scaled_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wUN6wimkA9Hc"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HnWbn51vBCJb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create Train and Test datasets\n",
        "Train = scaled_data\n",
        "# Save the Test dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wFcyNPTWBDb3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "# Step 10: Create TimeseriesGenerator for training data\n",
        "n_input = 24  # Use the last 24 hours for prediction\n",
        "n_features = 2  # Number of features (NO_OF_ADULT and NO_OF_CHILD)\n",
        "\n",
        "generator = TimeseriesGenerator(Train, Train, length=n_input, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPY9MUVKBFzF",
        "outputId": "9c714242-31e2-4010-9e17-29f39efcfc9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-10 14:31:02.620944: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "/home/jeev/.local/lib/python3.10/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Define the LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(units=100, activation='tanh', return_sequences=True), input_shape=(n_input, n_features)),\n",
        "    Bidirectional(LSTM(units=50, activation='tanh')),\n",
        "    Dense(units=n_features, activation='linear')  # 'linear' is appropriate for regression\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cMZqjnwEBKow"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kAQ2q1yBL1x",
        "outputId": "983e0f9f-e020-4709-c66c-5d01c3d79c43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jeev/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 0.0292 - mae: 0.1245\n",
            "Epoch 2/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.0107 - mae: 0.0735\n",
            "Epoch 3/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - loss: 0.0073 - mae: 0.0564\n",
            "Epoch 4/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 0.0066 - mae: 0.0576\n",
            "Epoch 5/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 0.0053 - mae: 0.0522\n",
            "Epoch 6/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0052 - mae: 0.0512\n",
            "Epoch 7/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 24ms/step - loss: 0.0051 - mae: 0.0498\n",
            "Epoch 8/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - loss: 0.0047 - mae: 0.0459\n",
            "Epoch 9/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0043 - mae: 0.0451\n",
            "Epoch 10/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - loss: 0.0035 - mae: 0.0398\n",
            "Epoch 11/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0039 - mae: 0.0432\n",
            "Epoch 12/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - loss: 0.0037 - mae: 0.0416\n",
            "Epoch 13/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0041 - mae: 0.0439\n",
            "Epoch 14/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0044 - mae: 0.0436\n",
            "Epoch 15/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0033 - mae: 0.0388\n",
            "Epoch 16/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0036 - mae: 0.0417\n",
            "Epoch 17/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 21ms/step - loss: 0.0034 - mae: 0.0406\n",
            "Epoch 18/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0032 - mae: 0.0382\n",
            "Epoch 19/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0026 - mae: 0.0341\n",
            "Epoch 20/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0027 - mae: 0.0350\n",
            "Epoch 21/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0030 - mae: 0.0368\n",
            "Epoch 22/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0023 - mae: 0.0333\n",
            "Epoch 23/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0029 - mae: 0.0362\n",
            "Epoch 24/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0031 - mae: 0.0368\n",
            "Epoch 25/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - loss: 0.0027 - mae: 0.0375\n",
            "Epoch 26/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0033 - mae: 0.0383\n",
            "Epoch 27/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0024 - mae: 0.0334\n",
            "Epoch 28/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0026 - mae: 0.0347\n",
            "Epoch 29/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0019 - mae: 0.0300\n",
            "Epoch 30/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0022 - mae: 0.0322\n",
            "Epoch 31/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - loss: 0.0022 - mae: 0.0325\n",
            "Epoch 32/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - loss: 0.0017 - mae: 0.0285\n",
            "Epoch 33/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - loss: 0.0020 - mae: 0.0308\n",
            "Epoch 34/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - loss: 0.0016 - mae: 0.0282\n",
            "Epoch 35/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0020 - mae: 0.0310\n",
            "Epoch 36/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0019 - mae: 0.0300\n",
            "Epoch 37/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0022 - mae: 0.0323\n",
            "Epoch 38/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0017 - mae: 0.0278\n",
            "Epoch 39/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0015 - mae: 0.0270\n",
            "Epoch 40/40\n",
            "\u001b[1m597/597\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - loss: 0.0024 - mae: 0.0330\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# model training\n",
        "# Train the model\n",
        "epochs = 40  # More epochs can lead to better performance\n",
        "\n",
        "# Add EarlyStopping callback to stop training if the validation loss does not improve\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with the EarlyStopping callback\n",
        "history = model.fit(generator, epochs=epochs, verbose=1, callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IBTRq2gjIrBx"
      },
      "outputs": [],
      "source": [
        "model.save(month+'.keras')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
