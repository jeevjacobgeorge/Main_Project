{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from pyspark.sql.functions import concat, to_timestamp, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/12/27 10:16:42 WARN Utils: Your hostname, J4Hp resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/27 10:16:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/27 10:16:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/27 10:16:57 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as spark_sum, col, hour, concat_ws, to_date, date_format\n",
    "# Stop any existing Spark session\n",
    "# Step 1: Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataProcessing\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "# Step 2: Load your CSV file into a Spark DataFrame\n",
    "data = spark.read.csv(\"June2024.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, concat, col, lit, date_format, expr\n",
    "# Filter rows where ROUTE_ID is 'acwXkRFM'\n",
    "# data = data.filter(col(\"ROUTE_ID\") == 'acwXkRFM')\n",
    "# Step 2: Format TICKET_ISSUE_TIME as a string in \"HH:mm:ss\" format (if not already) and combine date and time\n",
    "data = data.withColumn(\"TICKET_ISSUE_TIME_STR\", date_format(col(\"TICKET_ISSUE_TIME\"), \"HH:mm:ss\"))\n",
    "data = data.withColumn(\"TICKET_DATETIME_STR\", concat(col(\"TICKET_ISSUE_DATE\"), lit(\" \"), col(\"TICKET_ISSUE_TIME_STR\")))\n",
    "\n",
    "# Step 3: Convert the combined date-time string to a timestamp format\n",
    "data = data.withColumn(\"TICKET_DATETIME\", to_timestamp(\"TICKET_DATETIME_STR\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "# Step 4: Round TICKET_DATETIME to the nearest 10-minute interval\n",
    "data = data.withColumn(\"TICKET_DATETIME_15MIN\", expr(\"date_trunc('minute', TICKET_DATETIME) + INTERVAL 15 MINUTE * floor(minute(TICKET_DATETIME) / 15)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group by TICKET_DATETIME, FROM_STOP_NAME, and TO_STOP_NAME, and sum the TOTAL_PASSENGER\n",
    "aggregated_data_from = (\n",
    "    data.groupBy(\"TICKET_DATETIME_15MIN\", \"FROM_STOP_NAME\")\n",
    "    .sum(\"TOTAL_PASSENGER\")\n",
    "    .withColumnRenamed(\"sum(TOTAL_PASSENGER)\", \"TOTAL_PASSENGER\")\n",
    ")\n",
    "# Sort by TICKET_DATETIME_5MIN\n",
    "aggregated_data_from = aggregated_data_from.orderBy(\"TICKET_DATETIME_15MIN\")\n",
    "# aggregated_data_from.show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install folium geopy pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import folium\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from folium.plugins import MarkerCluster\n",
    "import time\n",
    "\n",
    "# Define the specific time range\n",
    "start_time = \"2024-06-01 04:45:00\"\n",
    "end_time = \"2024-06-01 12:45:00\"\n",
    "\n",
    "# Aggregate data to get total passenger count per bus stop within the time range\n",
    "# Sort by total passengers in descending order and select the top 20\n",
    "top_bus_stops = (\n",
    "    aggregated_data_from.groupBy(\"FROM_STOP_NAME\")\n",
    "    .agg(F.sum(\"TOTAL_PASSENGER\").alias(\"TOTAL_PASSENGER\"))\n",
    "    .orderBy(\"TOTAL_PASSENGER\", ascending=False)\n",
    "    .limit(1000)\n",
    "    .collect()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geocoding Successes: 550\n",
      "Geocoding Failures: 450\n",
      "Failed to geocode the following bus stops:\n",
      "['Statue Sbi (outside South India)', 'Chakai (outside South India)', 'World Market (outside South India)', 'Rotary (outside South India)', '16th Mile (outside South India)', 'Mamam (outside South India)', 'Titanium (outside South India)', 'Mananthala (outside South India)', 'Maithani (outside South India)', 'Sainik School (outside South India)', 'St Andrews (outside South India)', 'Veli Church (outside South India)', 'Thrikkannapuram (outside South India)', 'Azhikode (outside South India)', 'Saraswathy Vidyalaya School (outside South India)', 'Killy (outside South India)', 'Museum (outside South India)', 'Karipur (outside South India)', 'Bakery Junction (outside South India)', 'Statue (outside South India)', 'Kaithakkadu (outside South India)', 'Agricultural College (outside South India)', 'Concordia Lutheran High School (outside South India)', 'Pachira (outside South India)', 'J T S Mancha (outside South India)', 'Kanakkod (outside South India)', 'Scooter Factory (outside South India)', 'Parappil (outside South India)', '28th Mile (outside South India)', 'Aramada (outside South India)', 'Mahadevapuram (outside South India)', 'Thaliyil Temple (outside South India)', 'Mayam (outside South India)', 'Koothali (outside South India)', 'Kappil (outside South India)', 'Villant (outside South India)', 'Elamba (outside South India)', 'Bhagat Singh Road (outside South India)', 'Lords (outside South India)', 'Vattayam (outside South India)', 'Fci (outside South India)', 'Doordarshan (outside South India)', 'Concordia School (outside South India)', '6th Stone (outside South India)', 'Pump House (outside South India)', 'Technical H S S (outside South India)', 'Ar Nagar (outside South India)', 'Koottappara (outside South India)', '10th Stone (outside South India)', 'Kurumi (outside South India)', 'Lulu Mall (outside South India)', \"St Mary's School (outside South India)\", 'Devi Nagar (outside South India)', 'Dpi (outside South India)', 'Pantha (outside South India)', 'Kanakakunnu (outside South India)', 'Nilama (outside South India)', 'St Joseph Church (outside South India)', 'Sabari (outside South India)', 'Gc Nagar (outside South India)', 'Medical College (outside South India)', 'Kuthali (outside South India)', 'Chitra Nagar (outside South India)', 'Manavari (outside South India)']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Latitude and Longitude bounds for South India\n",
    "SOUTH_INDIA_LAT_MIN = 8.0\n",
    "SOUTH_INDIA_LAT_MAX = 14.5\n",
    "SOUTH_INDIA_LON_MIN = 76.0\n",
    "SOUTH_INDIA_LON_MAX = 85.0\n",
    "\n",
    "# File to store previously geocoded bus stops\n",
    "GEO_CACHE_FILE = 'geocoded_stops.json'\n",
    "FAILURE_CACHE_FILE = 'geocoding_failures.json'\n",
    "# Function to load previously cached geocoded data from a JSON file\n",
    "def load_geocoded_data():\n",
    "    try:\n",
    "        with open(GEO_CACHE_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "# Function to save geocoded data to a JSON file\n",
    "def save_geocoded_data(data):\n",
    "    with open(GEO_CACHE_FILE, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Function to check if a location is in South India\n",
    "def is_in_south_india(latitude, longitude):\n",
    "    return (SOUTH_INDIA_LAT_MIN <= latitude <= SOUTH_INDIA_LAT_MAX) and (SOUTH_INDIA_LON_MIN <= longitude <= SOUTH_INDIA_LON_MAX)\n",
    "\n",
    "def load_geocoded_failures():\n",
    "    try:\n",
    "        with open(FAILURE_CACHE_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "# Prepare data for geocoding (replace 'top_bus_stops' with your actual data)\n",
    "bus_stops_data = [{\"stop_name\": row[\"FROM_STOP_NAME\"], \"passenger_count\": row[\"TOTAL_PASSENGER\"]} for row in top_bus_stops]\n",
    "\n",
    "# Initialize the geocoder\n",
    "geolocator = Nominatim(user_agent=\"bus_stop_locator\")\n",
    "\n",
    "# Load previously geocoded data from the cache\n",
    "cached_data = load_geocoded_data()\n",
    "cached_failure_data = load_geocoded_failures()\n",
    "# Initialize counters for success and failure\n",
    "success_count = 0\n",
    "failure_count = 0\n",
    "failures = []\n",
    "\n",
    "# Geocode each bus stop with a timeout and sleep to avoid too many requests\n",
    "for stop in bus_stops_data:\n",
    "    stop_name = stop[\"stop_name\"]\n",
    "    # Skip if the stop is in the failures list\n",
    "    \n",
    "    # Check if the stop has already been geocoded\n",
    "    if stop_name in cached_data:\n",
    "        # Use cached data\n",
    "        stop[\"latitude\"] = cached_data[stop_name][\"latitude\"]\n",
    "        stop[\"longitude\"] = cached_data[stop_name][\"longitude\"]\n",
    "        success_count += 1  # Increment success count for cached data\n",
    "        continue  # Skip geocoding since it's already cached\n",
    "    elif stop_name in cached_failure_data:\n",
    "        # Skip geocoding if it previously failed\n",
    "        failure_count += 1\n",
    "        continue\n",
    "    # If not cached, geocode this stop\n",
    "    try:\n",
    "        # Modify the stop name query to limit geocoding to South India\n",
    "        location = geolocator.geocode(f\"{stop_name}\", timeout=20)\n",
    "        \n",
    "        if location:\n",
    "            latitude = location.latitude\n",
    "            longitude = location.longitude\n",
    "\n",
    "            # Check if the coordinates are within South India's bounds\n",
    "            if is_in_south_india(latitude, longitude):\n",
    "                stop[\"latitude\"] = latitude\n",
    "                stop[\"longitude\"] = longitude\n",
    "                # Save the geocoded result in the cache\n",
    "                cached_data[stop_name] = {\"latitude\": stop[\"latitude\"], \"longitude\": stop[\"longitude\"]}\n",
    "                success_count += 1  # Increment success count\n",
    "            else:\n",
    "                # If outside South India, mark as None\n",
    "                stop[\"latitude\"] = None\n",
    "                stop[\"longitude\"] = None\n",
    "                failure_count += 1  # Increment failure count\n",
    "                failures.append(f\"{stop_name} (outside South India)\")\n",
    "        else:\n",
    "            stop[\"latitude\"] = None\n",
    "            stop[\"longitude\"] = None\n",
    "            failure_count += 1  # Increment failure count\n",
    "            failures.append(stop_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding {stop_name}: {e}\")\n",
    "        stop[\"latitude\"] = None\n",
    "        stop[\"longitude\"] = None\n",
    "        failures.append(stop_name)\n",
    "        failure_count += 1  # Increment failure count\n",
    "\n",
    "    # Pause for 1 second between requests to avoid hitting API rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save the updated geocoded data to the cache file\n",
    "save_geocoded_data(cached_data)\n",
    "\n",
    "# Output the number of successes and failures\n",
    "print(f\"Geocoding Successes: {success_count}\")\n",
    "print(f\"Geocoding Failures: {failure_count}\")\n",
    "if failures:\n",
    "    print(\"Failed to geocode the following bus stops:\")\n",
    "    print(failures)\n",
    "    # File to store geocoding failures\n",
    "    \n",
    "\n",
    "    # Function to save geocoding failures to a JSON file\n",
    "    def save_geocoding_failures(failures):\n",
    "        with open(FAILURE_CACHE_FILE, 'w') as f:\n",
    "            json.dump(failures, f, indent=4)\n",
    "\n",
    "    # Save the geocoding failures to the cache file\n",
    "    save_geocoding_failures(failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "import folium\n",
    "import pandas as pd\n",
    "from folium import Icon\n",
    "\n",
    "# Filter out stops without coordinates\n",
    "stops_with_coords = [stop for stop in bus_stops_data if \"latitude\" in stop and \"longitude\" in stop and stop[\"latitude\"] is not None and stop[\"longitude\"] is not None]\n",
    "\n",
    "# Convert to Pandas DataFrame for easier handling with Folium\n",
    "stops_df = pd.DataFrame(stops_with_coords)\n",
    "\n",
    "# Initialize a Folium map centered around an average location\n",
    "map_center = [8.4869, 76.9529]\n",
    "m = folium.Map(location=map_center, tiles=\"\", zoom_start=13, min_zoom=5,max_zoom=20)\n",
    "\n",
    "# Prepare data for HeatMap (latitude, longitude, and intensity)\n",
    "heat_data = []\n",
    "for _, row in stops_df.iterrows():\n",
    "    heat_data.append([row[\"latitude\"], row[\"longitude\"], row[\"passenger_count\"]])\n",
    "\n",
    "# Create the HeatMap layer with enhanced visual settings\n",
    "HeatMap(\n",
    "    heat_data,\n",
    "    min_opacity=0.3,  # Increase minimum opacity for better visibility\n",
    "    max_opacity=0.9,  # Increase maximum opacity for better visibility\n",
    "    radius=20,        # Increase size of heat spots\n",
    "    blur=20,          # Increase amount of blur applied\n",
    "    gradient={        # Enhanced gradient color scale\n",
    "        0.2: 'blue',  # Low passenger count -> blue\n",
    "        0.4: 'cyan',\n",
    "        0.6: 'lime',\n",
    "        0.8: 'orange',\n",
    "        1.0: 'red',   # High passenger count -> red\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Create a MarkerCluster to manage markers based on zoom level\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# Add popups for bus stops with their name and passenger count\n",
    "for _, row in stops_df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "        popup=f\"<b>{row['stop_name']}</b><br>Passenger count: {row['passenger_count']}\",\n",
    "        tooltip=row[\"stop_name\"],\n",
    "        icon=Icon(icon_size=(10, 10))  # Adjust the icon size here\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save map to an HTML file\n",
    "m.save(\"passenger_density_map_with_clusters.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from folium.plugins import HeatMap\n",
    "# Filter out stops without coordinates\n",
    "stops_with_coords = [stop for stop in bus_stops_data if stop[\"latitude\"] is not None and stop[\"longitude\"] is not None]\n",
    "\n",
    "# Convert to Pandas DataFrame for easier handling with Folium\n",
    "stops_df = pd.DataFrame(stops_with_coords)\n",
    "\n",
    "# Initialize a Folium map centered around an average location\n",
    "map_center = [8.4869, 76.9529]\n",
    "m = folium.Map(location=map_center,tiles=\"OpenStreetMap\" ,zoom_start=13,min_zoom=5,max_zoom=15)\n",
    "# Prepare data for HeatMap (latitude, longitude, and intensity)\n",
    "heat_data = []\n",
    "for _, row in stops_df.iterrows():\n",
    "    heat_data.append([row[\"latitude\"], row[\"longitude\"], row[\"passenger_count\"]])\n",
    "\n",
    "# Create the HeatMap layer\n",
    "HeatMap(\n",
    "    heat_data,\n",
    "    min_opacity=0.2,  # Minimum opacity (low-intensity areas will be more transparent)\n",
    "    max_opacity=0.8,  # Maximum opacity (high-intensity areas will be more visible)\n",
    "    radius=20,        # Adjust size of heat spots\n",
    "    blur=15,          # Amount of blur applied\n",
    "    gradient={        # Gradient color scale\n",
    "        0.2: 'blue',  # Low passenger count -> blue\n",
    "        0.4: 'green',\n",
    "        0.6: 'yellow',\n",
    "        0.8: 'red',   # High passenger count -> red\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add popups for bus stops with their name and passenger count\n",
    "for _, row in stops_df.iterrows():\n",
    "    marker = folium.Marker(\n",
    "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "        popup=f\"<b>{row['stop_name']}</b><br>Passenger count: {row['passenger_count']}\",\n",
    "        tooltip=row[\"stop_name\"],\n",
    "    )\n",
    "\n",
    "    # Add zoom level restriction for the marker\n",
    "    marker.add_to(m)\n",
    "    marker._parent.options['maxZoom'] = 15  # Adjust to the max zoom level when marker becomes visible\n",
    "    marker._parent.options['minZoom'] = 10  # Adjust to the min zoom level for when marker is hidden\n",
    "\n",
    "# Save map to an HTML file\n",
    "m.save(\"passenger_density_map.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
